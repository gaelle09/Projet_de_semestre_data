{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE VALIDE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all functions\n",
    "\n",
    "# Step 1: Fetch domains with their meanings\n",
    "def fetch_domains():\n",
    "    url = \"https://api.archives-ouvertes.fr/ref/domain/?wt=xml&q=level_i:%220%22\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    domains = []\n",
    "    if response.status_code == 200:\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        for domain in root.findall(\".//doc\"):\n",
    "            label = domain.find(\".//str[@name='label_s']\")\n",
    "            if label is not None:\n",
    "                domain_info = label.text.split(\" = \")\n",
    "                if len(domain_info) == 2:\n",
    "                    domain_code, domain_meaning = domain_info\n",
    "                    domains.append((domain_code, domain_meaning))\n",
    "    else:\n",
    "        print(f\"Error fetching domains: {response.status_code}\")\n",
    "    return domains\n",
    "\n",
    "\n",
    "def fetch_test_data_curl_2021(domain_code):\n",
    "    # Create the output directory based on domain\n",
    "    output_directory = f\"Test_pos/2021/{domain_code}\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_file = f\"{output_directory}/test_data_pos_ART_2021.json\"\n",
    "\n",
    "    # Construct the API URL\n",
    "    url = (\n",
    "        f\"https://api.archives-ouvertes.fr/search/?q=level0_domain_s:\\\"{domain_code}\\\"\"\n",
    "        f\"&fq=docType_s:\\\"ART\\\"\"\n",
    "        f\"&fq=language_t:\\\"fr\\\"\"\n",
    "        f\"&fq=-fr_abstract_s:\\\"\\\"\"\n",
    "        f\"&fq=-fr_abstract_s:\\\"None\\\"\"\n",
    "        f\"&fq=fr_abstract_s:*\"\n",
    "        #f\"&fq=fileMain_s:*\"\n",
    "        f\"&fq=submittedDateY_i:2021\"\n",
    "        f\"&fl=docType_t,level0_domain_s,docid,title_s,fr_abstract_s,fileMain_s\"\n",
    "        f\"&rows=50&wt=json\"\n",
    "    )\n",
    "\n",
    "    # Curl command to fetch data and save it to the specified output file\n",
    "    curl_command = f\"curl -s '{url}' -o {output_file}\"\n",
    "\n",
    "    # Run the curl command\n",
    "    subprocess.run(curl_command, shell=True)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "\n",
    "def fetch_test_data_curl_2022(domain_code):\n",
    "    # Create the output directory based on domain\n",
    "    output_directory = f\"Test_pos/2022/{domain_code}\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    output_file = f\"{output_directory}/test_data_pos_ART_2022.json\"\n",
    "\n",
    "    # Construct the API URL\n",
    "    url = (\n",
    "        f\"https://api.archives-ouvertes.fr/search/?q=level0_domain_s:\\\"{domain_code}\\\"\"\n",
    "        f\"&fq=docType_s:\\\"ART\\\"\"\n",
    "        f\"&fq=language_t:\\\"fr\\\"\"\n",
    "        f\"&fq=-fr_abstract_s:\\\"\\\"\"\n",
    "        f\"&fq=-fr_abstract_s:\\\"None\\\"\"\n",
    "        f\"&fq=fr_abstract_s:*\"\n",
    "        #f\"&fq=fileMain_s:*\"\n",
    "        f\"&fq=submittedDateY_i:2022\"\n",
    "        f\"&fq=-submittedDateM_i:12\"\n",
    "        f\"&fl=docType_t,level0_domain_s,docid,title_s,fr_abstract_s,fileMain_s\"\n",
    "        f\"&rows=50&wt=json\"\n",
    "    )\n",
    "\n",
    "    # Curl command to fetch data and save it to the specified output file\n",
    "    curl_command = f\"curl -s '{url}' -o {output_file}\"\n",
    "\n",
    "    # Run the curl command\n",
    "    subprocess.run(curl_command, shell=True)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "\n",
    "def fetch_domain_test_data_curl_2021(domain_list):\n",
    "    for domain_code, _ in domain_list:\n",
    "        fetch_test_data_curl_2021(domain_code)\n",
    "\n",
    "def fetch_domain_test_data_curl_2022(domain_list):\n",
    "    for domain_code, _ in domain_list:\n",
    "        fetch_test_data_curl_2022(domain_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the domains\n",
    "domains_1 = fetch_domains()\n",
    "domains_2 = fetch_domains() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING AND CURLING DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_domain_test_data_curl_2021(domains_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_domain_test_data_curl_2022(domains_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEAN AND FILTER RESULTS TO ONLY KEEP ONES WITH AN URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def clean_and_filter_results(base_directory, output_directory):\n",
    "    # Iterate over years (e.g., 2021, 2022)\n",
    "    for date in os.listdir(base_directory):\n",
    "        date_path = os.path.join(base_directory, date)\n",
    "        if not os.path.isdir(date_path):\n",
    "            continue\n",
    "\n",
    "        # Iterate over domain codes (e.g., shs, sdv, spi)\n",
    "        for domain_code in os.listdir(date_path):\n",
    "            domain_path = os.path.join(date_path, domain_code)\n",
    "            if not os.path.isdir(domain_path):\n",
    "                continue\n",
    "\n",
    "            # Process each JSON file in the domain folder\n",
    "            for filename in os.listdir(domain_path):\n",
    "                if not filename.endswith('.json'):\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(domain_path, filename)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "                    print(f\"Error reading {filename}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                docs = data.get('response', {}).get('docs', [])\n",
    "                if not docs:\n",
    "                    print(f\"No documents found in {filename}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                seen_docids = set()\n",
    "                filtered_docs = []\n",
    "\n",
    "                # Filter docs with 'fileMain_s' and remove duplicates\n",
    "                for doc in docs:\n",
    "                    docid = doc.get('docid')\n",
    "                    if not docid or docid in seen_docids:\n",
    "                        continue\n",
    "                    if 'fileMain_s' in doc:\n",
    "                        filtered_docs.append(doc)\n",
    "                        seen_docids.add(docid)\n",
    "                    if len(filtered_docs) == 20:\n",
    "                        break\n",
    "\n",
    "                # Update and save the cleaned results\n",
    "                if filtered_docs:\n",
    "                    data['response']['docs'] = filtered_docs\n",
    "                    data['response']['numFound'] = len(filtered_docs)\n",
    "\n",
    "                    output_folder = os.path.join(output_directory, date, domain_code)\n",
    "                    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                    output_file = os.path.join(output_folder, filename)\n",
    "                    try:\n",
    "                        with open(output_file, 'w') as f:\n",
    "                            json.dump(data, f, indent=4)\n",
    "                        print(f\"Updated and saved: {output_file}\")\n",
    "                    except IOError as e:\n",
    "                        print(f\"Error writing to {output_file}: {e}\")\n",
    "                else:\n",
    "                    print(f\"No valid results with 'fileMain_s' in {filename}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base and output directories\n",
    "base_dir = 'Test_pos'\n",
    "output_dir = 'Cleaned_Test_pos'\n",
    "clean_and_filter_results(base_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWNLOAD DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_document(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download from {url}: {e}\")\n",
    "\n",
    "def download_documents(input_directory, download_directory):\n",
    "    os.makedirs(download_directory, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if not filename.endswith('.json'):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Failed to read {filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            docs = data.get('response', {}).get('docs', [])\n",
    "            for doc in docs:\n",
    "                docid = doc.get('docid')\n",
    "                file_url = doc.get('fileMain_s')\n",
    "\n",
    "                if not file_url:\n",
    "                    continue\n",
    "\n",
    "                save_path = os.path.join(download_directory, f\"{docid}.pdf\")\n",
    "                download_document(file_url, save_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_dir = 'Cleaned_Test_pos'\n",
    "download_dir = 'Downloaded_Documents'\n",
    "download_documents(input_dir, download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def download_document(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {save_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download from {url}: {e}\")\n",
    "\n",
    "def download_documents(input_directory, download_directory):\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if not filename.endswith('.json'):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Failed to read {filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            docs = data.get('response', {}).get('docs', [])\n",
    "            for doc in docs:\n",
    "                docid = doc.get('docid')\n",
    "                file_url = doc.get('fileMain_s')\n",
    "\n",
    "                if not file_url:\n",
    "                    continue\n",
    "\n",
    "                # Extract year and domain from the directory structure\n",
    "                parts = root.split(os.sep)\n",
    "                year, domain_code = parts[-2], parts[-1]\n",
    "\n",
    "                # Create the corresponding output directory\n",
    "                output_path = os.path.join(download_directory, year, domain_code)\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "                save_path = os.path.join(output_path, f\"{docid}.pdf\")\n",
    "                download_document(file_url, save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_dir = 'Cleaned_Test_pos'\n",
    "download_dir = 'Downloaded_Documents'\n",
    "download_documents(input_dir, download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST NOUGAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pymupdf python-Levenshtein nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/huggingface/transformers.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/private/var/folders/0d/c55v27zd62j542ptl29_r8840000gq/T/\u001b[0m\u001b[32mpip-req-build-bcp5wzxd\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[5 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m error: RPC failed; curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)\n",
      "  \u001b[31m   \u001b[0m error: 2146 bytes of body are still expected\n",
      "  \u001b[31m   \u001b[0m fetch-pack: unexpected disconnect while reading sideband packet\n",
      "  \u001b[31m   \u001b[0m fatal: early EOF\n",
      "  \u001b[31m   \u001b[0m fatal: fetch-pack: invalid index-pack output\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/huggingface/transformers.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/private/var/folders/0d/c55v27zd62j542ptl29_r8840000gq/T/\u001b[0m\u001b[32mpip-req-build-bcp5wzxd\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import torch\n",
    "from transformers import AutoProcessor, VisionEncoderDecoderModel\n",
    "from huggingface_hub import snapshot_download\n",
    "import re\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Download the NOUGAT model locally\n",
    "local_dir = snapshot_download(repo_id=\"facebook/nougat-small\")\n",
    "\n",
    "# Load the model and processor from the local directory\n",
    "processor = AutoProcessor.from_pretrained(local_dir)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(local_dir)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT CONTENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaelle/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load NOUGAT model from Hugging Face\n",
    "model_name = \"facebook/nougat-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def extract_content_with_nougat(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            content = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    content += page_text + \"\\n\"\n",
    "\n",
    "        # Split content into smaller chunks for NOUGAT\n",
    "        chunk_size = 512\n",
    "        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "\n",
    "        extracted_text = \"\"\n",
    "        for chunk in chunks:\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True)\n",
    "            outputs = model.generate(**inputs)\n",
    "            extracted_chunk = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            extracted_text += extracted_chunk + \"\\n\"\n",
    "\n",
    "        # Remove \"Résumé\" or \"Abstract\" sections\n",
    "        extracted_text = re.sub(r\"(?i)(Résumé|Abstract)[:\\-]*[^\\n]*(\\n|$)\", \"\", extracted_text)\n",
    "\n",
    "        return extracted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_pdfs_with_nougat(input_directory, output_directory):\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if not filename.endswith('.pdf'):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(root, filename)\n",
    "            cleaned_content = extract_content_with_nougat(file_path)\n",
    "\n",
    "            if cleaned_content:\n",
    "                # Maintain directory structure\n",
    "                parts = root.split(os.sep)\n",
    "                year, domain_code = parts[-2], parts[-1]\n",
    "                output_path = os.path.join(output_directory, year, domain_code)\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "                output_file = os.path.join(output_path, f\"{os.path.splitext(filename)[0]}_content.txt\")\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned_content)\n",
    "                print(f\"Processed and saved content to: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_dir = 'Downloaded_Documents'\n",
    "output_dir = 'Processed_Documents'\n",
    "process_pdfs_with_nougat(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaelle/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load NOUGAT model from Hugging Face\n",
    "model_name = \"facebook/nougat-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def extract_content_with_nougat(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            content = \"\"\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    content += page_text + \"\\n\"\n",
    "\n",
    "        # Split content into smaller chunks for NOUGAT\n",
    "        chunk_size = 512\n",
    "        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "\n",
    "        extracted_text = \"\"\n",
    "        for chunk in chunks:\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True)\n",
    "            outputs = model.generate(**inputs)\n",
    "            extracted_chunk = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            extracted_text += extracted_chunk + \"\\n\"\n",
    "\n",
    "        # Remove \"Résumé\" or \"Abstract\" sections\n",
    "        extracted_text = re.sub(r\"(?i)(Résumé|Abstract)[:\\-]*[^\\n]*(\\n|$)\", \"\", extracted_text)\n",
    "\n",
    "        return extracted_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_pdfs_with_nougat(input_directory, output_directory):\n",
    "    for root, _, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if not filename.endswith('.pdf'):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(root, filename)\n",
    "            cleaned_content = extract_content_with_nougat(file_path)\n",
    "\n",
    "            if cleaned_content:\n",
    "                # Maintain directory structure\n",
    "                parts = root.split(os.sep)\n",
    "                year, domain_code = parts[-2], parts[-1]\n",
    "                output_path = os.path.join(output_directory, year, domain_code)\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "                output_file = os.path.join(output_path, f\"{os.path.splitext(filename)[0]}_content.txt\")\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned_content)\n",
    "                print(f\"Processed and saved content to: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_dir = 'Downloaded_Documents'\n",
    "output_dir = 'Processed_Documents'\n",
    "process_pdfs_with_nougat(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
